# Import necessary libraries
import nltk
import pandas as pd
import numpy as np
nltk.download('punkt')
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.stem import PorterStemmer, WordNetLemmatizer
nltk.download('wordnet')
import string
!pip install contractions
import contractions
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, recall_score, f1_score

# Load the dataset
data = pd.read_csv('Tweets.csv')
text = data['text']
sentiment = data['airline_sentiment']

# Initialize text processing tools
stemmer = PorterStemmer()
lemma = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

# Define the text preprocessing function
def preprocess(text):
    text = text.lower()  # Convert text to lowercase
    text = contractions.fix(text)  # Expand contractions
    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation
    tokens = text.split()  # Split text into tokens
    tokens = [lemma.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatize and remove stopwords
    return ' '.join(tokens)  # Rejoin tokens into a single string

# Apply preprocessing to the text column
text = text.apply(preprocess)

# Vectorize the text using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(text)
y = sentiment

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Initialize and train the logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred, average="weighted")
print("Accuracy_score:", accuracy)
print("f1_score:", f1)
